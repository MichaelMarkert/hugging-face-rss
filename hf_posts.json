{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Kseniase/113319295427497", "image": "", "title": "8 New Types of RAG", "content_text": "8 New Types of RAG RAG techniques continuously evolve to enhance LLM response accuracy by retrieving relevant external data during generation. To keep up with current AI trends, new RAG types incorporate deep step-by-step reasoning, tree search, citations, multimodality and other effective techniques. Here's a list of 8 latest RAG advancements: 1. DeepRAG -> DeepRAG: Thinking to Retrieval Step by Step for Large Language Models (2502.01142) Models retrieval-augmented reasoning as a Markov Decision Process, enabling strategic retrieval. It dynamically decides when to retrieve external knowledge and when rely on parametric reasoning. 2. RealRAG -> RealRAG: Retrieval-augmented Realistic Image Generation via Self-reflective Contrastive Learning (2502.00848) Enhances novel object generation by retrieving real-world images and using self-reflective contrastive learning to fill knowledge gap, improve realism and reduce distortions. 3. Chain-of-Retrieval Augmented Generation (CoRAG) ->...", "url": "https://huggingface.co/posts/Kseniase/113319295427497", "date_published": "2025-02-11T13:26:53.721061"}, {"id": "https://huggingface.co/posts/schuler/395413718646507", "image": "", "title": "\ud83d\udce2 New Research Alert: Making Language Models Smaller & Smarter!", "content_text": "\ud83d\udce2 New Research Alert: Making Language Models Smaller & Smarter! Thrilled to share the latest technical report demonstrating how to reduce language model parameters by 77% while maintaining performance. The secret? Grouped pointwise convolutions. Yes. We brought a method from computer vision to the transformers arena. \ud83d\udd11 Key Findings: \u2022 77% parameter reduction. \u2022 Maintained model capabilities. \u2022 Improved generalization. Paper: https://www.researchgate.net/publication/388835829_SAVING_77_OF_THE_PARAMETERS_IN_LARGE_LANGUAGE_MODELS_TECHNICAL_REPORT Code: https://github.com/joaopauloschuler/less-parameters-llm See translation", "url": "https://huggingface.co/posts/schuler/395413718646507", "date_published": "2025-02-11T13:26:53.721433"}, {"id": "https://huggingface.co/posts/ginipick/776720011919298", "image": "", "title": "Time Stream \u23f3\ud83d\ude80", "content_text": "Time Stream \u23f3\ud83d\ude80 Time Stream is a groundbreaking AI tool that transforms your text into a mesmerizing video journey from the past to the future. With this innovative technology, your ideas evolve over time, visualized through a dynamic image strip and a fluid video narrative. Imagine typing a simple prompt and watching as your words transform into vivid scenes that capture every moment of change\u2014like a time machine for creativity! \ud83c\udfa5\u2728 Key Features: \u2022 Text-to-Video Transformation: Enter any text, and Time Stream converts it into a compelling video that travels through time, turning your ideas into a visual story. \ud83d\udcfd\ufe0f \u2022 Dynamic Image Strip: Alongside the video, a vibrant image strip is created, showcasing each stage of the transformation so you can see every detail of the evolution. \ud83d\udcf8 \u2022 Customizable Settings: Adjust parameters such as strength, guidance scale, and more to fine-tune your video\u2019s appearance and ensure it perfectly matches your creative vision. \u2699\ufe0f \u2022 User-Friendly Interface:...", "url": "https://huggingface.co/posts/ginipick/776720011919298", "date_published": "2025-02-11T13:26:53.722025"}, {"id": "https://huggingface.co/posts/s-emanuilov/736266652835078", "image": "", "title": "Tutorial \ud83d\udca5 Training a non-English reasoning model with GRPO and Unsloth", "content_text": "Tutorial \ud83d\udca5 Training a non-English reasoning model with GRPO and Unsloth I wanted to share my experiment with training reasoning models in languages other than English/Chinese. Using Llama 3.1 8B as base, GRPO trainer from trl, and Unsloth optimizations, I got a working prototype in Bulgarian after ~5 hours on an L40S GPU. The approach should work for any language where the base model has some pre-training coverage. Full code and tutorial here: https://unfoldai.com/reasoning-in-a-non-english-language/ The model itself: s-emanuilov/LLMBG-Llama-3.1-8B-BG-Reasoning-v0.1 I hope this helps anyone looking to build reasoning models in their language. See translation", "url": "https://huggingface.co/posts/s-emanuilov/736266652835078", "date_published": "2025-02-11T13:26:53.722393"}, {"id": "https://huggingface.co/posts/kadirnar/407959263704733", "image": "", "title": "Researchers developed Sonic AI enabling precise facial animation from speech cues \ud83c\udfa7 Decouples head/expression control via audio tone analysis + time-aware fusion for natural long-form synthesis", "content_text": "Researchers developed Sonic AI enabling precise facial animation from speech cues \ud83c\udfa7 Decouples head/expression control via audio tone analysis + time-aware fusion for natural long-form synthesis See translation", "url": "https://huggingface.co/posts/kadirnar/407959263704733", "date_published": "2025-02-11T13:26:53.722634"}, {"id": "https://huggingface.co/posts/singhsidhukuldeep/821835295778849", "image": "", "title": "Fascinating deep dive into Swiggy's Hermes - their in-house Text-to-SQL solution that's revolutionizing data accessibility!", "content_text": "Fascinating deep dive into Swiggy's Hermes - their in-house Text-to-SQL solution that's revolutionizing data accessibility! Hermes enables natural language querying within Slack, generating and executing SQL queries with an impressive <2 minute turnaround time. The system architecture is particularly intriguing: Technical Implementation: - Built on GPT-4 with a Knowledge Base + RAG approach for Swiggy-specific context - AWS Lambda middleware handles communication between Slack UI and the Gen AI model - Databricks jobs orchestrate query generation and execution Under the Hood: The pipeline employs a sophisticated multi-stage approach: 1. Metrics retrieval using embedding-based vector lookup 2. Table/column identification through metadata descriptions 3. Few-shot SQL retrieval with vector-based search 4. Structured prompt creation with data snapshots 5. Query validation with automated error correction Architecture Highlights: - Compartmentalized by business units (charters) for better...", "url": "https://huggingface.co/posts/singhsidhukuldeep/821835295778849", "date_published": "2025-02-11T13:26:53.723094"}, {"id": "https://huggingface.co/posts/lewtun/162100455547462", "image": "", "title": "Introducing OpenR1-Math-220k!", "content_text": "Introducing OpenR1-Math-220k! open-r1/OpenR1-Math-220k The community has been busy distilling DeepSeek-R1 from inference providers, but we decided to have a go at doing it ourselves from scratch \ud83d\udcaa What\u2019s new compared to existing reasoning datasets? \u267e Based on AI-MO/NuminaMath-1.5 : we focus on math reasoning traces and generate answers for problems in NuminaMath 1.5, an improved version of the popular NuminaMath-CoT dataset. \ud83d\udc33 800k R1 reasoning traces: We generate two answers for 400k problems using DeepSeek R1. The filtered dataset contains 220k problems with correct reasoning traces. \ud83d\udcc0 512 H100s running locally: Instead of relying on an API, we leverage vLLM and SGLang to run generations locally on our science cluster, generating 180k reasoning traces per day. \u23f3 Automated filtering: We apply Math Verify to only retain problems with at least one correct answer. We also leverage Llama3.3-70B-Instruct as a judge to retrieve more correct examples (e.g for cases with malformed answers...", "url": "https://huggingface.co/posts/lewtun/162100455547462", "date_published": "2025-02-11T13:26:53.723599"}, {"id": "https://huggingface.co/posts/burtenshaw/457613029588941", "image": "", "title": "The Hugging Face agents course is finally out!", "content_text": "The Hugging Face agents course is finally out! \ud83d\udc49 https://huggingface.co/agents-course This first unit of the course sets you up with all the fundamentals to become a pro in agents. - What's an AI Agent? - What are LLMs? - Messages and Special Tokens - Understanding AI Agents through the Thought-Action-Observation Cycle - Thought, Internal Reasoning and the Re-Act Approach - Actions, Enabling the Agent to Engage with Its Environment - Observe, Integrating Feedback to Reflect and Adapt See translation", "url": "https://huggingface.co/posts/burtenshaw/457613029588941", "date_published": "2025-02-11T13:26:53.723981"}, {"id": "https://huggingface.co/posts/prithivMLmods/964278651693422", "image": "", "title": "QwQ Edge Gets a Small Update..! \ud83d\udcac", "content_text": "QwQ Edge Gets a Small Update..! \ud83d\udcac try now: prithivMLmods/QwQ-Edge \ud83d\ude80Now, you can use the following commands for different tasks: \ud83d\uddbc\ufe0f @ image 'prompt...' \u2192 Generates an image \ud83d\udd09@tts1 'prompt...' \u2192 Generates speech in a female voice \ud83d\udd09 @ tts2 'prompt...' \u2192 Generates speech in a male voice \ud83c\udd70\ufe0f@text 'prompt...' \u2192 Enables textual conversation (If not specified, text-to-text generation is the default mode) \ud83d\udcacMultimodality Support : prithivMLmods/Qwen2-VL-OCR-2B-Instruct \ud83d\udcacFor text generation, the FastThink-0.5B model ensures quick and efficient responses, prithivMLmods/FastThink-0.5B-Tiny \ud83d\udcacImage Generation: sdxl lightning model, SG161222/RealVisXL_V4.0_Lightning Github: https://github.com/PRITHIVSAKTHIUR/QwQ-Edge graph TD A[User Interface] --> B[Chat Logic] B --> C{Command Type } C -->| Text | D [FastThink -0.5 B] C -->| Image | E [Qwen2-VL-OCR -2 B] C -->| @image | F [Stable Diffusion XL] C -->| @tts | G [Edge TTS] D --> H[Response] E --> H F --> H G --> H See translation", "url": "https://huggingface.co/posts/prithivMLmods/964278651693422", "date_published": "2025-02-11T13:26:53.724445"}, {"id": "https://huggingface.co/posts/burtenshaw/334573649974058", "image": "", "title": "We\u2019re launching a FREE and CERTIFIED course on Agents!", "content_text": "We\u2019re launching a FREE and CERTIFIED course on Agents! We're thrilled to announce the launch of the Hugging Face Agents course on Learn! This interactive, certified course will guide you through building and deploying your own AI agents. Here's what you'll learn: - Understanding Agents: We'll break down the fundamentals of AI agents, showing you how they use LLMs to perceive their environment (observations), reason about it (thoughts), and take actions. Think of a smart assistant that can book appointments, answer emails, or even write code based on your instructions. - Building with Frameworks: You'll dive into popular agent frameworks like LangChain, LlamaIndex and smolagents. These tools provide the building blocks for creating complex agent behaviors. - Real-World Applications: See how agents are used in practice, from automating SQL queries to generating code and summarizing complex documents. - Certification: Earn a certification by completing the course modules, implementing...", "url": "https://huggingface.co/posts/burtenshaw/334573649974058", "date_published": "2025-02-11T13:26:53.724986"}]}