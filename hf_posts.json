{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Kseniase/113319295427497", "image": "", "title": "8 New Types of RAG", "content_text": "8 New Types of RAG RAG techniques continuously evolve to enhance LLM response accuracy by retrieving relevant external data during generation. To keep up with current AI trends, new RAG types incorporate deep step-by-step reasoning, tree search, citations, multimodality and other effective techniques. Here's a list of 8 latest RAG advancements: 1. DeepRAG -> DeepRAG: Thinking to Retrieval Step by Step for Large Language Models (2502.01142) Models retrieval-augmented reasoning as a Markov Decision Process, enabling strategic retrieval. It dynamically decides when to retrieve external knowledge and when rely on parametric reasoning. 2. RealRAG -> RealRAG: Retrieval-augmented Realistic Image Generation via Self-reflective Contrastive Learning (2502.00848) Enhances novel object generation by retrieving real-world images and using self-reflective contrastive learning to fill knowledge gap, improve realism and reduce distortions. 3. Chain-of-Retrieval Augmented Generation (CoRAG) ->...", "url": "https://huggingface.co/posts/Kseniase/113319295427497", "date_published": "2025-02-10T13:26:50.410732"}, {"id": "https://huggingface.co/posts/schuler/395413718646507", "image": "", "title": "\ud83d\udce2 New Research Alert: Making Language Models Smaller & Smarter!", "content_text": "\ud83d\udce2 New Research Alert: Making Language Models Smaller & Smarter! Thrilled to share the latest technical report demonstrating how to reduce language model parameters by 77% while maintaining performance. The secret? Grouped pointwise convolutions. Yes. We brought a method from computer vision to the transformers arena. \ud83d\udd11 Key Findings: \u2022 77% parameter reduction. \u2022 Maintained model capabilities. \u2022 Improved generalization. Paper: https://www.researchgate.net/publication/388835829_SAVING_77_OF_THE_PARAMETERS_IN_LARGE_LANGUAGE_MODELS_TECHNICAL_REPORT Code: https://github.com/joaopauloschuler/less-parameters-llm See translation", "url": "https://huggingface.co/posts/schuler/395413718646507", "date_published": "2025-02-10T13:26:50.411104"}, {"id": "https://huggingface.co/posts/s-emanuilov/736266652835078", "image": "", "title": "Tutorial \ud83d\udca5 Training a non-English reasoning model with GRPO and Unsloth", "content_text": "Tutorial \ud83d\udca5 Training a non-English reasoning model with GRPO and Unsloth I wanted to share my experiment with training reasoning models in languages other than English/Chinese. Using Llama 3.1 8B as base, GRPO trainer from trl, and Unsloth optimizations, I got a working prototype in Bulgarian after ~5 hours on an L40S GPU. The approach should work for any language where the base model has some pre-training coverage. Full code and tutorial here: https://unfoldai.com/reasoning-in-a-non-english-language/ The model itself: s-emanuilov/LLMBG-Llama-3.1-8B-BG-Reasoning-v0.1 I hope this helps anyone looking to build reasoning models in their language. See translation", "url": "https://huggingface.co/posts/s-emanuilov/736266652835078", "date_published": "2025-02-10T13:26:50.411471"}, {"id": "https://huggingface.co/posts/Xenova/620657830533509", "image": "", "title": "We did it. Kokoro TTS (v1.0) can now run 100% locally in your browser w/ WebGPU acceleration. Real-time text-to-speech without a server. \u26a1\ufe0f", "content_text": "We did it. Kokoro TTS (v1.0) can now run 100% locally in your browser w/ WebGPU acceleration. Real-time text-to-speech without a server. \u26a1\ufe0f Generate 10 seconds of speech in ~1 second for $0. What will you build? \ud83d\udd25 webml-community/kokoro-webgpu The most difficult part was getting the model running in the first place, but the next steps are simple: \u2702\ufe0f Implement sentence splitting, allowing for streamed responses \ud83c\udf0d Multilingual support (only phonemization left) Who wants to help? See translation", "url": "https://huggingface.co/posts/Xenova/620657830533509", "date_published": "2025-02-10T13:26:50.411799"}, {"id": "https://huggingface.co/posts/prithivMLmods/964278651693422", "image": "", "title": "QwQ Edge Gets a Small Update..! \ud83d\udcac", "content_text": "QwQ Edge Gets a Small Update..! \ud83d\udcac try now: prithivMLmods/QwQ-Edge \ud83d\ude80Now, you can use the following commands for different tasks: \ud83d\uddbc\ufe0f @ image 'prompt...' \u2192 Generates an image \ud83d\udd09@tts1 'prompt...' \u2192 Generates speech in a female voice \ud83d\udd09 @ tts2 'prompt...' \u2192 Generates speech in a male voice \ud83c\udd70\ufe0f@text 'prompt...' \u2192 Enables textual conversation (If not specified, text-to-text generation is the default mode) \ud83d\udcacMultimodality Support : prithivMLmods/Qwen2-VL-OCR-2B-Instruct \ud83d\udcacFor text generation, the FastThink-0.5B model ensures quick and efficient responses, prithivMLmods/FastThink-0.5B-Tiny \ud83d\udcacImage Generation: sdxl lightning model, SG161222/RealVisXL_V4.0_Lightning Github: https://github.com/PRITHIVSAKTHIUR/QwQ-Edge graph TD A[User Interface] --> B[Chat Logic] B --> C{Command Type } C -->| Text | D [FastThink -0.5 B] C -->| Image | E [Qwen2-VL-OCR -2 B] C -->| @image | F [Stable Diffusion XL] C -->| @tts | G [Edge TTS] D --> H[Response] E --> H F --> H G --> H See translation", "url": "https://huggingface.co/posts/prithivMLmods/964278651693422", "date_published": "2025-02-10T13:26:50.412278"}, {"id": "https://huggingface.co/posts/hexgrad/846477530846098", "image": "", "title": "Wanted: Peak Data. I'm collecting audio data to train another TTS model:", "content_text": "Wanted: Peak Data. I'm collecting audio data to train another TTS model: + AVM data: ChatGPT Advanced Voice Mode audio & text from source + Professional audio: Permissive (CC0, Apache, MIT, CC-BY) This audio should *impress* most native speakers, not just barely pass their audio Turing tests. Professional-caliber means S or A-tier, not your average bloke off the street. Traditional TTS may not make the cut. Absolutely no low-fi microphone recordings like Common Voice. The bar is much higher than last time, so there are no timelines yet and I expect it may take longer to collect such mythical data. Raising the bar means evicting quite a bit of old data, and voice/language availability may decrease. The theme is *quality* over quantity. I would rather have 1 hour of A/S-tier than 100 hours of mid data. I have nothing to offer but the north star of a future Apache 2.0 TTS model, so prefer data that you *already have* and costs you *nothing extra* to send. Additionally, *all* the new...", "url": "https://huggingface.co/posts/hexgrad/846477530846098", "date_published": "2025-02-10T13:26:50.412720"}, {"id": "https://huggingface.co/posts/openfree/664271513735189", "image": "", "title": "VectorFlow \u26a1: Transform Images into Professional Vector Graphics", "content_text": "VectorFlow \u26a1: Transform Images into Professional Vector Graphics Convert your raster images (JPG, PNG, WEBP) into high-quality vector graphics (SVG, AI) with our easy-to-use tool! Perfect for designers, artists, and anyone needing vector conversions. \ud83c\udfaf Key Features: Dual format support: SVG and AI output Real-time preview for both formats Advanced customization options Clean, user-friendly interface Batch processing ready \ud83d\udee0\ufe0f Advanced Controls: Color/B&W mode selection Speckle filtering Color precision adjustment Layer management Curve fitting options \ud83d\udcab Why VectorFlow? No installation needed Free to use Professional-grade output Simple yet powerful \ud83d\udd27 Technical Details: Built with Gradio Powered by VTracer Optimized SVG generation AI format support \ud83d\udc49 Try it now: openfree/VectorFlow #computervision #vectorgraphics #imageprocessing #svg #design #ai See translation", "url": "https://huggingface.co/posts/openfree/664271513735189", "date_published": "2025-02-10T13:26:50.413150"}, {"id": "https://huggingface.co/posts/ginipick/845644282975973", "image": "", "title": "\ud83c\udf1f 3D Llama Studio - AI 3D Generation Platform", "content_text": "\ud83c\udf1f 3D Llama Studio - AI 3D Generation Platform \ud83d\udcdd Project Overview 3D Llama Studio is an all-in-one AI platform that generates high-quality 3D models and stylized images from text or image inputs. \u2728 Key Features Text/Image to 3D Conversion \ud83c\udfaf Generate 3D models from detailed text descriptions or reference images Intuitive user interface Text to Styled Image Generation \ud83c\udfa8 Customizable image generation settings Adjustable resolution, generation steps, and guidance scale Supports both English and Korean prompts \ud83d\udee0\ufe0f Technical Features Gradio-based web interface Dark theme UI/UX Real-time image generation and 3D modeling \ud83d\udcab Highlights User-friendly interface Real-time preview Random seed generation High-resolution output support (up to 2048x2048) \ud83c\udfaf Applications Product design Game asset creation Architectural visualization Educational 3D content \ud83d\udd17 Try It Now! Experience 3D Llama Studio: ginigen/3D-LLAMA #AI #3DGeneration #MachineLearning #ComputerVision #DeepLearning See translation", "url": "https://huggingface.co/posts/ginipick/845644282975973", "date_published": "2025-02-10T13:26:50.413603"}, {"id": "https://huggingface.co/posts/singhsidhukuldeep/821835295778849", "image": "", "title": "Fascinating deep dive into Swiggy's Hermes - their in-house Text-to-SQL solution that's revolutionizing data accessibility!", "content_text": "Fascinating deep dive into Swiggy's Hermes - their in-house Text-to-SQL solution that's revolutionizing data accessibility! Hermes enables natural language querying within Slack, generating and executing SQL queries with an impressive <2 minute turnaround time. The system architecture is particularly intriguing: Technical Implementation: - Built on GPT-4 with a Knowledge Base + RAG approach for Swiggy-specific context - AWS Lambda middleware handles communication between Slack UI and the Gen AI model - Databricks jobs orchestrate query generation and execution Under the Hood: The pipeline employs a sophisticated multi-stage approach: 1. Metrics retrieval using embedding-based vector lookup 2. Table/column identification through metadata descriptions 3. Few-shot SQL retrieval with vector-based search 4. Structured prompt creation with data snapshots 5. Query validation with automated error correction Architecture Highlights: - Compartmentalized by business units (charters) for better...", "url": "https://huggingface.co/posts/singhsidhukuldeep/821835295778849", "date_published": "2025-02-10T13:26:50.414043"}, {"id": "https://huggingface.co/posts/nicolay-r/658571348150271", "image": "", "title": "\ud83d\udce2 If you wish to empower LLM with IR and named entity recognition module, then I got relevant findings.", "content_text": "\ud83d\udce2 If you wish to empower LLM with IR and named entity recognition module, then I got relevant findings. Just tested Flair below is how you can start for adapting for processing your CSV / JSONL data via bulk-ner \ud83d\udc69\u200d\ud83d\udcbb code: https://github.com/nicolay-r/nlp-thirdgate/blob/master/tutorials/ner_flair_0151.sh \ud83e\udd16 models: https://huggingface.co/flair Provider: https://raw.githubusercontent.com/nicolay-r/nlp-thirdgate/refs/heads/master/ner/flair_0151.py Framework: https://github.com/nicolay-r/bulk-ner \ud83d\ude80 Performance: the default ner model (Thinkpad X1 Nano) Batch-size 1 6it/sec Batch-size 10+ 12it/sec \ud83c\udf0c other wrappers for bulk-ner nlp-thirdgate: https://github.com/nicolay-r/nlp-thirdgate See translation", "url": "https://huggingface.co/posts/nicolay-r/658571348150271", "date_published": "2025-02-10T13:26:50.414398"}]}