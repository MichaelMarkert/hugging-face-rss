{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/m-ric/410805194640777", "title": "Introducing \ud835\uddfc\ud835\uddfd\ud835\uddf2\ud835\uddfb \ud835\uddd7\ud835\uddf2\ud835\uddf2\ud835\uddfd-\ud835\udde5\ud835\uddf2\ud835\ude00\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddf0\ud835\uddf5 by Hugging Face! \ud83d\udca5", "content_text": "Introducing \ud835\uddfc\ud835\uddfd\ud835\uddf2\ud835\uddfb \ud835\uddd7\ud835\uddf2\ud835\uddf2\ud835\uddfd-\ud835\udde5\ud835\uddf2\ud835\ude00\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddf0\ud835\uddf5 by Hugging Face! \ud83d\udca5\nOpenAI's latest agentic app Deep Research seems really good... But it's closed, as usual.\n\u23f1\ufe0f So with a team of cracked colleagues, we set ourselves a 24hours deadline to replicate and open-source Deep Research! \u23f1\ufe0f\n\u27a1\ufe0f We built open-Deep-Research, an entirely open agent that can: navigate the web autonomously, scroll and search through pages, download and manipulate files, run calculation on data...\nWe aimed for the best performance: are the agent's answers really rigorous?\nOn GAIA benchmark, Deep Research had 67% accuracy on the validation set.\n\u27a1\ufe0f open Deep Research is at 55% (powered by o1), it is:\n- the best pass@1 solution submitted\n- the best open solution \ud83d\udcaa\ud83d\udcaa\nAnd it's only getting started ! Please jump in, drop PRs, and let's bring it to the top !\nRead the blog post \ud83d\udc49\nhttps://huggingface.co/blog/open-deep-research\nSee translation", "url": "https://huggingface.co/posts/m-ric/410805194640777", "date_published": "2025-02-05T17:05:37.246119"}, {"id": "https://huggingface.co/posts/ahmed-masry/423590538819866", "title": "Happy to announce AlignVLM \ud83d\udccf \u2013 a novel approach to bridging vision and language latent spaces for multimodal understanding in Vision-Language Models (VLMs) \ud83c\udf0d\ud83d\udcc4\ud83d\uddbc", "content_text": "Happy to announce AlignVLM \ud83d\udccf \u2013 a novel approach to bridging vision and language latent spaces for multimodal understanding in Vision-Language Models (VLMs) \ud83c\udf0d\ud83d\udcc4\ud83d\uddbc\n\ud83d\udd17 Read the paper:\nAlignVLM: Bridging Vision and Language Latent Spaces for Multimodal\n  Understanding (2502.01341)\n\ud83e\uddd0 What\u2019s the challenge?\nAligning visual features with language embeddings remains a major bottleneck in VLMs. Existing connectors such as Multi-layer perceptron (MLPs) often introduce noise that degrades performance. \u274c\n\ud83c\udfaf Our Solution: ALIGN Connector\nWe propose AlignVLM, a method that maps vision features into a weighted average of LLM text embeddings, ensuring they remain in a space that the LLM can effectively interpret. \u2705\n\ud83d\udd2c How does it perform?\nWe compared ALIGN against common connectors like MLPs, Perceiver Resampler, and Ovis trained under similar configurations. The results? ALIGN outperforms them all \ud83c\udfc6 on diverse document understanding tasks \ud83d\udcc4.\n\ud83d\udcca Meet the AlignVLM Model Family!\nWe trained Llama 3.1 (1B, 3B, 8B) using our connector and benchmarked them against various models. The results:\n\u2705 AlignVLM surpasses all Base VLMs trained under similar configurations. \u2705 Our models also perform competitively against Instruct VLMs such as Qwen2-VL and InternVL-2.5 \ud83d\ude80.\n\ud83e\udd14 What about robustness to noise?\nWe injected Gaussian noise (\u03bc=0, \u03c3=3) into the vision encoder\u2019s outputs before feeding them to the connector:\n\u2705 ALIGN Connector: Minimal drop (\u21931.67%) \u2013 proving its high robustness!\n\u274c MLP Connector: Severe degradation (\u219325.54%) \u2013 struggling with noisy inputs.\nCode & model weights coming soon! Stay tuned! \ud83d\udd25\nSee translation", "url": "https://huggingface.co/posts/ahmed-masry/423590538819866", "date_published": "2025-02-05T17:05:37.246125"}, {"id": "https://huggingface.co/posts/oleggolev/371041427101015", "title": "\ud83d\ude80 Dobby-mini is out!", "content_text": "\ud83d\ude80 Dobby-mini is out!\nLast week,\n@\nSentientAGI\nreleased two demo models for the upcoming Dobby model family which we are building with your feedback:\nSentientAGI/dobby-mini-679af3ed45dfdd8c25e8112c\n\ud83d\udd25 The two models (available as transformers and GGUF) are here:\n-\nSentientAGI/Dobby-Mini-Unhinged-Llama-3.1-8B\n\ud83d\ude08\n-\nSentientAGI/Dobby-Mini-Leashed-Llama-3.1-8B\n\ud83d\ude07\nFine-tuned from Llama-3.1-8B-Instruct while retaining benchmark performance, these personality-enhanced models are prime for building anything from AI companions and social agents to opinionated chatbots and content generators.\n- \ud83e\udd85 Pro-freedom\n- \ud83d\udcb8 Pro-crypto\n- \ud83d\udcaa Opinionated and stand their ground\n\ud83d\udcbb Local Setup with Ollama:\n- Written instructions:\nhttps://huggingface.co/blog/chrisaubin/hosting-dobby-mini\n- Companion video:\nhttps://www.youtube.com/watch?v=b1rbtCgK2YA\n\ud83c\udf86 Use via API on Fireworks for free!\n- Unhinged:\nhttps://tinyurl.com/4h2c7tmv\n- Leashed:\nhttps://tinyurl.com/2xjwsdxb\n\u270c\ufe0f Try Dobby-mini via a Gradio demo:\n-\nhttps://demo-dobby.sentient.xyz/\n- No Internet search, ask it some personal questions!\nDobby-70B en route \ud83d\ude0e\nSee translation", "url": "https://huggingface.co/posts/oleggolev/371041427101015", "date_published": "2025-02-05T17:05:37.246126"}, {"id": "https://huggingface.co/posts/Jaward/491908622353726", "title": "ByteDance drops OmniHuman\ud83d\udd25", "content_text": "ByteDance drops OmniHuman\ud83d\udd25\nThis is peak SOTA performance - flawless natural gestures with perfect lip sync and facial expressions. This is the second time they've released SOTA level talking-heads only this time with hands and body motion.\nProject:\nhttps://omnihuman-lab.github.io/\nSee translation", "url": "https://huggingface.co/posts/Jaward/491908622353726", "date_published": "2025-02-05T17:05:37.246128"}, {"id": "https://huggingface.co/posts/victor/435864388294574", "title": "Hey everyone, we've given", "content_text": "Hey everyone, we've given\nhttps://hf.co/spaces\npage a fresh update!\nSmart Search: Now just type what you want to do\u2014like \"make a viral meme\" or \"generate music\"\u2014and our search gets it.\nNew Categories: Check out the cool new filter bar with icons to help you pick a category fast.\nRedesigned Space Cards: Reworked a bit to really show off the app descriptions, so you know what each Space does at a glance.\nRandom Prompt: Need ideas? Hit the dice button for a burst of inspiration.\nWe\u2019d love to hear what you think\u2014drop us some feedback plz!\nSee translation", "url": "https://huggingface.co/posts/victor/435864388294574", "date_published": "2025-02-05T17:05:37.246129"}, {"id": "https://huggingface.co/posts/singhsidhukuldeep/759130196829951", "title": "Exciting Research Alert: Revolutionizing Complex Information Retrieval!", "content_text": "Exciting Research Alert: Revolutionizing Complex Information Retrieval!\nA groundbreaking paper from researchers at MIT, AWS AI, and UPenn introduces ARM (Alignment-Oriented LLM-based Retrieval Method), a novel approach to tackle complex information retrieval challenges.\n>> Key Innovations\nInformation Alignment\nThe method first decomposes queries into keywords and aligns them with available data using both BM25 and embedding similarity, ensuring comprehensive coverage of information needs.\nStructure Alignment\nARM employs a sophisticated mixed-integer programming solver to identify connections between data objects, exploring relationships beyond simple semantic matching.\nSelf-Verification\nThe system includes a unique self-verification mechanism where the LLM evaluates and aggregates results from multiple retrieval paths, ensuring accuracy and completeness.\n>> Performance Highlights\nThe results are impressive:\n- Outperforms standard RAG by up to 5.2 points in execution accuracy on Bird dataset\n- Achieves 19.3 points higher F1 scores compared to existing approaches on OTT-QA\n- Reduces the number of required LLM calls while maintaining superior retrieval quality\n>> Technical Implementation\nThe system uses a three-step process:\n1. N-gram indexing and embedding computation for all data objects\n2. Constrained beam decoding for information alignment\n3. Mixed-integer programming optimization for structure exploration\nThis research represents a significant step forward in making complex information retrieval more efficient and accurate. The team's work demonstrates how combining traditional optimization techniques with modern LLM capabilities can solve challenging retrieval problems.\nSee translation", "url": "https://huggingface.co/posts/singhsidhukuldeep/759130196829951", "date_published": "2025-02-05T17:05:37.246130"}, {"id": "https://huggingface.co/posts/rubenroy/210189348112698", "title": "\ud83d\udd25\ud83d\ude80  Hey everyone! I'm excited to share my latest LLM release: Gilgamesh 72B, a model built on Qwen 2.5-72B Instruct. Gilgamesh was trained on a couple of my GammaCorpus datasets, specifically:", "content_text": "\ud83d\udd25\ud83d\ude80  Hey everyone! I'm excited to share my latest LLM release: Gilgamesh 72B, a model built on Qwen 2.5-72B Instruct. Gilgamesh was trained on a couple of my GammaCorpus datasets, specifically:\n-\nrubenroy/GammaCorpus-CoT-Math-170k\n-\nrubenroy/GammaCorpus-v2-5m\n-\nrubenroy/GammaCorpus-Fact-QA-450k\nI've submitted GGM 72B to the Open LLM Leaderboard for benchmarking, I'll send an update post once the results are in!\nYou can try it out and share your feedback, check out the model page and see what it can do:\n\ud83d\udc49\nrubenroy/Gilgamesh-72B\nWould love to hear your thoughts!\nSee translation", "url": "https://huggingface.co/posts/rubenroy/210189348112698", "date_published": "2025-02-05T17:05:37.246131"}, {"id": "https://huggingface.co/posts/davidberenstein1957/544355915253284", "title": "Anyone can create free hosted tools for their AI agents! \ud83d\udd25", "content_text": "Anyone can create free hosted tools for their AI agents! \ud83d\udd25\nAgentic RAG stack part 2 - augment\nAugment retrieval results by reranking optimises content without increasing time too much\npart2:\nhttps://huggingface.co/blog/davidberenstein1957/ai-blueprint-agentic-rag-part-2-augment\ncode:\nhttps://github.com/huggingface/ai-blueprint\nSee translation", "url": "https://huggingface.co/posts/davidberenstein1957/544355915253284", "date_published": "2025-02-05T17:05:37.246134"}, {"id": "https://huggingface.co/posts/albertvillanova/425238785566061", "title": "\ud83d\ude80 Introducing", "content_text": "\ud83d\ude80 Introducing\n@\nhuggingface\nOpen Deep-Research\ud83d\udca5\nIn just 24 hours, we built an open-source agent that:\n\u2705 Autonomously browse the web\n\u2705 Search, scroll & extract info\n\u2705 Download & manipulate files\n\u2705 Run calculations on data\n55% on GAIA validation set! Help us improve it!\ud83d\udca1\nhttps://huggingface.co/blog/open-deep-research\nSee translation", "url": "https://huggingface.co/posts/albertvillanova/425238785566061", "date_published": "2025-02-05T17:05:37.246135"}, {"id": "https://huggingface.co/posts/Tonic/203849177096667", "title": "\ud83d\ude4b\ud83c\udffb\u200d\u2642\ufe0fhey there folks ,", "content_text": "\ud83d\ude4b\ud83c\udffb\u200d\u2642\ufe0fhey there folks ,\nGoedel's Theorem Prover is now being demo'ed on huggingface :\nTonic/Math\ngive it a try !\nSee translation", "url": "https://huggingface.co/posts/Tonic/203849177096667", "date_published": "2025-02-05T17:05:37.246137"}]}