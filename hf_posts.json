{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/m-ric/410805194640777", "title": "Introducing \ud835\uddfc\ud835\uddfd\ud835\uddf2\ud835\uddfb \ud835\uddd7\ud835\uddf2\ud835\uddf2\ud835\uddfd-\ud835\udde5\ud835\uddf2\ud835\ude00\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddf0\ud835\uddf5 by Hugging Face! \ud83d\udca5", "content_text": "Introducing \ud835\uddfc\ud835\uddfd\ud835\uddf2\ud835\uddfb \ud835\uddd7\ud835\uddf2\ud835\uddf2\ud835\uddfd-\ud835\udde5\ud835\uddf2\ud835\ude00\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddf0\ud835\uddf5 by Hugging Face! \ud83d\udca5\nOpenAI's latest agentic app Deep Research seems really good... But it's closed, as usual.\n\u23f1\ufe0f So with a team of cracked colleagues, we set ourselves a 24hours deadline to replicate and open-source Deep Research! \u23f1\ufe0f\n\u27a1\ufe0f We built open-Deep-Research, an entirely open agent that can: navigate the web autonomously, scroll and search through pages, download and manipulate files, run calculation on data...\nWe aimed for the best performance: are the agent's answers really rigorous?\nOn GAIA benchmark, Deep Research had 67% accuracy on the validation set.\n\u27a1\ufe0f open Deep Research is at 55% (powered by o1), it is:\n- the best pass@1 solution submitted\n- the best open solution \ud83d\udcaa\ud83d\udcaa\nAnd it's only getting started ! Please jump in, drop PRs, and let's bring it to the top !\nRead the blog post \ud83d\udc49\nhttps://huggingface.co/blog/open-deep-research\nSee translation", "url": "https://huggingface.co/posts/m-ric/410805194640777", "date_published": "2025-02-06T07:00:21.254671"}, {"id": "https://huggingface.co/posts/ahmed-masry/423590538819866", "title": "Happy to announce AlignVLM \ud83d\udccf \u2013 a novel approach to bridging vision and language latent spaces for multimodal understanding in Vision-Language Models (VLMs) \ud83c\udf0d\ud83d\udcc4\ud83d\uddbc", "content_text": "Happy to announce AlignVLM \ud83d\udccf \u2013 a novel approach to bridging vision and language latent spaces for multimodal understanding in Vision-Language Models (VLMs) \ud83c\udf0d\ud83d\udcc4\ud83d\uddbc\n\ud83d\udd17 Read the paper:\nAlignVLM: Bridging Vision and Language Latent Spaces for Multimodal\n  Understanding (2502.01341)\n\ud83e\uddd0 What\u2019s the challenge?\nAligning visual features with language embeddings remains a major bottleneck in VLMs. Existing connectors such as Multi-layer perceptron (MLPs) often introduce noise that degrades performance. \u274c\n\ud83c\udfaf Our Solution: ALIGN Connector\nWe propose AlignVLM, a method that maps vision features into a weighted average of LLM text embeddings, ensuring they remain in a space that the LLM can effectively interpret. \u2705\n\ud83d\udd2c How does it perform?\nWe compared ALIGN against common connectors like MLPs, Perceiver Resampler, and Ovis trained under similar configurations. The results? ALIGN outperforms them all \ud83c\udfc6 on diverse document understanding tasks \ud83d\udcc4.\n\ud83d\udcca Meet the AlignVLM Model Family!\nWe trained Llama 3.1 (1B, 3B, 8B) using our connector and benchmarked them against various models. The results:\n\u2705 AlignVLM surpasses all Base VLMs trained under similar configurations. \u2705 Our models also perform competitively against Instruct VLMs such as Qwen2-VL and InternVL-2.5 \ud83d\ude80.\n\ud83e\udd14 What about robustness to noise?\nWe injected Gaussian noise (\u03bc=0, \u03c3=3) into the vision encoder\u2019s outputs before feeding them to the connector:\n\u2705 ALIGN Connector: Minimal drop (\u21931.67%) \u2013 proving its high robustness!\n\u274c MLP Connector: Severe degradation (\u219325.54%) \u2013 struggling with noisy inputs.\nCode & model weights coming soon! Stay tuned! \ud83d\udd25\nSee translation", "url": "https://huggingface.co/posts/ahmed-masry/423590538819866", "date_published": "2025-02-06T07:00:21.254681"}, {"id": "https://huggingface.co/posts/victor/435864388294574", "title": "Hey everyone, we've given", "content_text": "Hey everyone, we've given\nhttps://hf.co/spaces\npage a fresh update!\nSmart Search: Now just type what you want to do\u2014like \"make a viral meme\" or \"generate music\"\u2014and our search gets it.\nNew Categories: Check out the cool new filter bar with icons to help you pick a category fast.\nRedesigned Space Cards: Reworked a bit to really show off the app descriptions, so you know what each Space does at a glance.\nRandom Prompt: Need ideas? Hit the dice button for a burst of inspiration.\nWe\u2019d love to hear what you think\u2014drop us some feedback plz!\nSee translation", "url": "https://huggingface.co/posts/victor/435864388294574", "date_published": "2025-02-06T07:00:21.254683"}, {"id": "https://huggingface.co/posts/Jaward/491908622353726", "title": "ByteDance drops OmniHuman\ud83d\udd25", "content_text": "ByteDance drops OmniHuman\ud83d\udd25\nThis is peak SOTA performance - flawless natural gestures with perfect lip sync and facial expressions. This is the second time they've released SOTA level talking-heads only this time with hands and body motion.\nProject:\nhttps://omnihuman-lab.github.io/\nSee translation", "url": "https://huggingface.co/posts/Jaward/491908622353726", "date_published": "2025-02-06T07:00:21.254684"}, {"id": "https://huggingface.co/posts/oleggolev/371041427101015", "title": "\ud83d\ude80 Dobby-mini is out!", "content_text": "\ud83d\ude80 Dobby-mini is out!\nLast week,\n@\nSentientAGI\nreleased two demo models for the upcoming Dobby model family which we are building with your feedback:\nSentientAGI/dobby-mini-679af3ed45dfdd8c25e8112c\n\ud83d\udd25 The two models (available as transformers and GGUF) are here:\n-\nSentientAGI/Dobby-Mini-Unhinged-Llama-3.1-8B\n\ud83d\ude08\n-\nSentientAGI/Dobby-Mini-Leashed-Llama-3.1-8B\n\ud83d\ude07\nFine-tuned from Llama-3.1-8B-Instruct while retaining benchmark performance, these personality-enhanced models are prime for building anything from AI companions and social agents to opinionated chatbots and content generators.\n- \ud83e\udd85 Pro-freedom\n- \ud83d\udcb8 Pro-crypto\n- \ud83d\udcaa Opinionated and stand their ground\n\ud83d\udcbb Local Setup with Ollama:\n- Written instructions:\nhttps://huggingface.co/blog/chrisaubin/hosting-dobby-mini\n- Companion video:\nhttps://www.youtube.com/watch?v=b1rbtCgK2YA\n\ud83c\udf86 Use via API on Fireworks for free!\n- Unhinged:\nhttps://tinyurl.com/4h2c7tmv\n- Leashed:\nhttps://tinyurl.com/2xjwsdxb\n\u270c\ufe0f Try Dobby-mini via a Gradio demo:\n-\nhttps://demo-dobby.sentient.xyz/\n- No Internet search, ask it some personal questions!\nDobby-70B en route \ud83d\ude0e\nSee translation", "url": "https://huggingface.co/posts/oleggolev/371041427101015", "date_published": "2025-02-06T07:00:21.254685"}, {"id": "https://huggingface.co/posts/lin-tan/763019179488759", "title": "\ud83d\ude80 Excited to share that our paper, \"SELP: Generating Safe and Efficient Task Plans for Robot Agents with Large Language Models\", has been accepted to #ICRA2025! \ud83d\udd17 Preprint:", "content_text": "\ud83d\ude80 Excited to share that our paper, \"SELP: Generating Safe and Efficient Task Plans for Robot Agents with Large Language Models\", has been accepted to #ICRA2025! \ud83d\udd17 Preprint:\nhttps://arxiv.org/pdf/2409.19471\nWe introduce SELP (Safe Efficient LLM Planner), a novel approach for generating plans that adhere to user-specified constraints while optimizing for time-efficient execution. By leveraging linear temporal logic (LTL) to interpret natural language commands, SELP effectively handles complex commands and long-horizon tasks. \ud83e\udd16\n\ud83d\udca1SELP presents three key insights:\n1\ufe0f\u20e3 Equivalence Voting: Ensures robust translations from natural language instructions into LTL specifications.\n2\ufe0f\u20e3 Constrained Decoding: Uses the generated LTL formula to guide the autoregressive inference of plans, ensuring the generated plans conform to the LTL.\n3\ufe0f\u20e3 Domain-Specific Fine-Tuning: Customizes LLMs for specific robotic tasks, boosting both safety and efficiency.\n\ud83d\udcca Experiment: Our experiments demonstrate SELP\u2019s effectiveness and generalizability across diverse tasks. In drone navigation, SELP outperforms state-of-the-art LLM planners by 10.8% in safety rate and by 19.8% in plan efficiency. For robot manipulation, SELP achieves a 20.4% improvement in safety rate.\n@\nyiwu\n@\njiang719\n#ICRA2025 #LLM #Robotics #Agent #LLMPlanner\nSee translation", "url": "https://huggingface.co/posts/lin-tan/763019179488759", "date_published": "2025-02-06T07:00:21.254687"}, {"id": "https://huggingface.co/posts/rubenroy/210189348112698", "title": "\ud83d\udd25\ud83d\ude80  Hey everyone! I'm excited to share my latest LLM release: Gilgamesh 72B, a model built on Qwen 2.5-72B Instruct. Gilgamesh was trained on a couple of my GammaCorpus datasets, specifically:", "content_text": "\ud83d\udd25\ud83d\ude80  Hey everyone! I'm excited to share my latest LLM release: Gilgamesh 72B, a model built on Qwen 2.5-72B Instruct. Gilgamesh was trained on a couple of my GammaCorpus datasets, specifically:\n-\nrubenroy/GammaCorpus-CoT-Math-170k\n-\nrubenroy/GammaCorpus-v2-5m\n-\nrubenroy/GammaCorpus-Fact-QA-450k\nI've submitted GGM 72B to the Open LLM Leaderboard for benchmarking, I'll send an update post once the results are in!\nYou can try it out and share your feedback, check out the model page and see what it can do:\n\ud83d\udc49\nrubenroy/Gilgamesh-72B\nWould love to hear your thoughts!\nSee translation", "url": "https://huggingface.co/posts/rubenroy/210189348112698", "date_published": "2025-02-06T07:00:21.254688"}, {"id": "https://huggingface.co/posts/davidberenstein1957/544355915253284", "title": "Anyone can create free hosted tools for their AI agents! \ud83d\udd25", "content_text": "Anyone can create free hosted tools for their AI agents! \ud83d\udd25\nAgentic RAG stack part 2 - augment\nAugment retrieval results by reranking optimises content without increasing time too much\npart2:\nhttps://huggingface.co/blog/davidberenstein1957/ai-blueprint-agentic-rag-part-2-augment\ncode:\nhttps://github.com/huggingface/ai-blueprint\nSee translation", "url": "https://huggingface.co/posts/davidberenstein1957/544355915253284", "date_published": "2025-02-06T07:00:21.254691"}, {"id": "https://huggingface.co/posts/albertvillanova/425238785566061", "title": "\ud83d\ude80 Introducing", "content_text": "\ud83d\ude80 Introducing\n@\nhuggingface\nOpen Deep-Research\ud83d\udca5\nIn just 24 hours, we built an open-source agent that:\n\u2705 Autonomously browse the web\n\u2705 Search, scroll & extract info\n\u2705 Download & manipulate files\n\u2705 Run calculations on data\n55% on GAIA validation set! Help us improve it!\ud83d\udca1\nhttps://huggingface.co/blog/open-deep-research\nSee translation", "url": "https://huggingface.co/posts/albertvillanova/425238785566061", "date_published": "2025-02-06T07:00:21.254692"}, {"id": "https://huggingface.co/posts/hexgrad/562263062112849", "title": "I wrote an article about G2P:", "content_text": "I wrote an article about G2P:\nhttps://hf.co/blog/hexgrad/g2p\nG2P is an underrated piece of small TTS models, like offensive linemen who do a bunch of work and get no credit.\nInstead of relying on explicit G2P, larger speech models implicitly learn this task by eating many thousands of hours of audio data. They often use a 500M+ parameter LLM at the front to predict latent audio tokens over a learned codebook, then decode these tokens into audio.\nKokoro instead relies on G2P preprocessing, is 82M parameters, and thus needs less audio to learn. Because of this, we can cherrypick high fidelity audio for training data, and deliver solid speech for those voices. In turn, this excellent audio quality & lack of background noise helps explain why Kokoro is very competitive in single-voice TTS Arenas.\nSee translation", "url": "https://huggingface.co/posts/hexgrad/562263062112849", "date_published": "2025-02-06T07:00:21.254694"}]}