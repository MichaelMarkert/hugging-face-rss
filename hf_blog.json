{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Blog", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_blog.json", "items": [{"id": "https://huggingface.co/blog/open-deep-research", "image": "https://huggingface.co/blog/assets/open-deep-research/thumbnail.png", "title": "Open-source DeepResearch \u2013 Freeing our search agents", "content_text": "Back to Articles Open-source DeepResearch \u2013 Freeing our search agents Published February 4, 2025 Update on GitHub Upvote 732 +726 m-ric Aymeric Roucher albertvillanova Albert Villanova del Moral merve Merve Noyan thomwolf Thomas Wolf clefourrier Cl\u00e9mentine Fourrier TLDR Table of Contents What are Agent frameworks and why they matter? The GAIA benchmark Building an open Deep Research Using a CodeAgent Making the right tools \ud83d\udee0\ufe0f Results \ud83c\udfc5 Community Reproductions Most important next steps TLDR Yesterday, OpenAI released Deep Research , a system that browses the web to summarize content and answer questions based on the summary. The system is impressive and blew our minds when we tried it for the first time. One of the main results in the blog post is a strong improvement of performances on the General AI Assistants benchmark (GAIA) , a benchmark we\u2019ve been playing with recently as well, where they successfully reached near 67% correct answers on 1-shot on average, and 47.6% on...", "url": "https://huggingface.co/blog/open-deep-research", "date_published": "2025-02-04T00:00:00"}, {"id": "https://huggingface.co/blog/pi0", "image": "https://huggingface.co/blog/assets/192_pi0/new_thumbnail_pi0.001.png", "title": "\u03c00 and \u03c00-FAST: Vision-Language-Action Models for General Robot Control", "content_text": "Back to Articles \u03c00 and \u03c00-FAST: Vision-Language-Action Models for General Robot Control Published February 4, 2025 Update on GitHub Upvote 77 +71 danaaubakirova Dana Aubakirova Molbap Pablo Montalvo mshukor Mustafa Shukor cadene Remi Cadene Introduction \ud83d\udd0d What is \u03c00? How to Use \u03c00 in LeRobot? Inference on \u03c00 pretrained model Fine-tuning the \u03c00 Pretrained Model What is the difference between VLMs and VLAs? Attention Mechanisms in Robotics Policies Key Idea \u26a1 Towards the Faster Attention in \u03c00 Handling 2D Attention Masks Can we use FlashAttention2? Using FlexAttention in PyTorch How to effectively represent Actions? \ud83d\ude80 What is \u03c00-FAST? Key Advantages of \u03c00-FAST: How does FAST work? How to use FAST tokenizer? What\u2019s Next for Generalist Robot Intelligence? Additional Resources References We have ported the first robotics foundation models to Hugging Face LeRobot ! Both \u03c00 and \u03c00-FAST , developed by Physical Intelligence, are now available in the LeRobot repository , bringing generalist...", "url": "https://huggingface.co/blog/pi0", "date_published": "2025-02-04T00:00:00"}, {"id": "https://huggingface.co/blog/dabstep", "image": "https://huggingface.co/blog/assets/dabstep/thumbnail.png", "title": "DABStep: Data Agent Benchmark for Multi-step Reasoning", "content_text": "Back to Articles DABStep: Data Agent Benchmark for Multi-step Reasoning Published February 4, 2025 Update on GitHub Upvote 32 +26 eggie5 Alex Egg guest martinigoyanes Martin Iglesias Goyanes guest frisokingma Friso Kingma guest lvwerra Leandro von Werra thomwolf Thomas Wolf Motivation Introducing DABstep What's inside the DABstep? Data Tasks Evaluations Real-time leaderboard Baselines Getting Started and Infra Future direction Related Works Language models are becoming increasingly capable and can solve tasks autonomously as agents. There are many exciting use cases, especially at the intersection of reasoning, code, and data. However, proper evaluation benchmarks on real-world problems are lacking and hinder progress in the field. To tackle this challenge, Adyen and Hugging Face built the Data Agent Benchmark for Multi-step Reasoning (DABstep) together. DABstep consists of over 450 data analysis tasks designed to evaluate the capabilities of state-of-the-art LLMs and AI agents. Our...", "url": "https://huggingface.co/blog/dabstep", "date_published": "2025-02-04T00:00:00"}, {"id": "https://huggingface.co/blog/ai-art-newsletter-jan-25", "image": "https://huggingface.co/blog/assets/ai_art_newsletter_1/thumbnail.png", "title": "The AI tools for Art Newsletter - Issue 1", "content_text": "Back to Articles The AI tools for Art Newsletter Published January 31, 2025 Update on GitHub Upvote 46 +40 linoyts Linoy Tsaban multimodalart Apolin\u00e1rio from multimodal AI art First issue \ud83c\udf89 Table of Contents Major Releases of 2024 Image Generation Text-to-image generation Personalization & stylization Video Generation Audio Generation Creative Tools that Shined in 2024 What should we expect for AI & Art in 2025? Starting off strong - Open source releases of January 25 Announcing Our Newsletter \ud83d\uddde\ufe0f The AI space is moving so fast it\u2019s hard to believe that a year ago we still struggled to generate people with the correct amount of fingers \ud83d\ude02. The last couple of years have been pivotal for open source models and tools for artistic usage. AI tools for creative expression have never been more accessible, and we\u2019re only scratching the surface. Join us as we look back at the key milestones, tools, and breakthroughs in AI & Arts from 2024, and forward for what\u2019s to come in 2025 (spoiler \ud83d\udc40:...", "url": "https://huggingface.co/blog/ai-art-newsletter-jan-25", "date_published": "2025-01-31T00:00:00"}, {"id": "https://huggingface.co/blog/deepseek-r1-aws", "image": "https://huggingface.co/blog/assets/deepseek-r1-aws/thumbnail.png", "title": "How to deploy and fine-tune DeepSeek models on AWS", "content_text": "Back to Articles How to deploy and fine-tune DeepSeek models on AWS Published January 30, 2025 Update on GitHub Upvote 35 +29 pagezyhf Simon Pagezy jeffboudier Jeff Boudier dacorvo David Corvoysier What is DeepSeek-R1? Deploy DeepSeek R1 models Deploy on AWS with Hugging Face Inference Endpoints Deploy on Amazon Sagemaker AI with Hugging Face LLM DLCs Deploy on EC2 Neuron with the Hugging Face Neuron Deep Learning AMI Fine-tune DeepSeek R1 models Fine tune on Amazon SageMaker AI with Hugging Face Training DLCs Fine tune on EC2 Neuron with the Hugging Face Neuron Deep Learning AMI A running document to showcase how to deploy and fine-tune DeepSeek R1 models with Hugging Face on AWS. What is DeepSeek-R1? If you\u2019ve ever struggled with a tough math problem, you know how useful it is to think a little longer and work through it carefully. OpenAI\u2019s o1 model showed that when LLMs are trained to do the same\u2014by using more compute during inference\u2014they get significantly better at solving...", "url": "https://huggingface.co/blog/deepseek-r1-aws", "date_published": "2025-01-30T00:00:00"}, {"id": "https://huggingface.co/blog/inference-providers", "image": "https://huggingface.co/blog/assets/inference-providers/thumbnail.png", "title": "Welcome to Inference Providers on the Hub \ud83d\udd25", "content_text": "Back to Articles Welcome to Inference Providers on the Hub \ud83d\udd25 Published January 28, 2025 Update on GitHub Upvote 275 +269 burkaygur Burkay Gur fal zeke Zeke Sikelianos replicate aton2006 Anton McGonnell sambanovasystems hassanelmghari Hassan El Mghari togethercomputer sbrandeis Simon Brandeis kramp Bertrand Chevrier julien-c Julien Chaumond How it works In the website UI From the client SDKs From HTTP calls Billing Feedback and next steps Today, we are launching the integration of four awesome serverless Inference Providers \u2013 fal, Replicate, Sambanova, Together AI \u2013 directly on the Hub\u2019s model pages. They are also seamlessly integrated into our client SDKs (for JS and Python), making it easier than ever to explore serverless inference of a wide variety of models that run on your favorite providers. We\u2019ve been hosting a serverless Inference API on the Hub for a long time (we launched the v1 in summer 2020 \u2013 wow, time flies \ud83e\udd2f). While this has enabled easy exploration and prototyping,...", "url": "https://huggingface.co/blog/inference-providers", "date_published": "2025-01-28T00:00:00"}, {"id": "https://huggingface.co/blog/open-r1", "image": "https://huggingface.co/blog/assets/open-r1/thumbnails.png", "title": "Open-R1: a fully open reproduction of DeepSeek-R1", "content_text": "Back to Articles Open-R1: a fully open reproduction of DeepSeek-R1 Published January 28, 2025 Update on GitHub Upvote 664 +658 eliebak Elie Bakouch lvwerra Leandro von Werra lewtun Lewis Tunstall What is DeepSeek-R1? How did they do it? Open-R1: the missing pieces What is DeepSeek-R1? If you\u2019ve ever struggled with a tough math problem, you know how useful it is to think a little longer and work through it carefully. OpenAI\u2019s o1 model showed that when LLMs are trained to do the same\u2014by using more compute during inference\u2014they get significantly better at solving reasoning tasks like mathematics, coding, and logic. However, the recipe behind OpenAI\u2019s reasoning models has been a well kept secret. That is, until last week, when DeepSeek released their DeepSeek-R1 model and promptly broke the internet (and the stock market! ). Besides performing as well or better than o1, the DeepSeek-R1 release was accompanied by a detailed tech report that outlined the key steps of their training...", "url": "https://huggingface.co/blog/open-r1", "date_published": "2025-01-28T00:00:00"}, {"id": "https://huggingface.co/blog/video_gen", "image": "https://huggingface.co/blog/assets/video_gen/thumbnail.png", "title": "State of open video generation models in Diffusers", "content_text": "Back to Articles State of open video generation models in Diffusers Published January 27, 2025 Update on GitHub Upvote 29 +23 sayakpaul Sayak Paul a-r-r-o-w Aryan V S dn6 Dhruv Nair Today\u2019s Video Generation Models and their Limitations Why is Video Generation Hard? Open Video Generation Models Video Generation with Diffusers Memory requirements Suite of optimizations Fine-tuning Looking ahead Resources OpenAI\u2019s Sora demo marked a striking advance in AI-generated video last year and gave us a glimpse of the potential capabilities of video generation models. The impact was immediate and since that demo, the video generation space has become increasingly competitive with major players and startups producing their own highly capable models such as Google\u2019s Veo2, Haliluo\u2019s Minimax, Runway\u2019s Gen3 Alpha, Kling, Pika, and Luma Lab\u2019s Dream Machine. Open-source has also had its own surge of video generation models with CogVideoX, Mochi-1, Hunyuan, Allegro, and LTX Video. Is the video...", "url": "https://huggingface.co/blog/video_gen", "date_published": "2025-01-27T00:00:00"}, {"id": "https://huggingface.co/blog/smolagents-can-see", "image": "https://huggingface.co/blog/assets/smolagents-can-see/thumbnail.png", "title": "We now support VLMs in smolagents!", "content_text": "Back to Articles We just gave sight to smolagents Published January 24, 2025 Update on GitHub Upvote 71 +65 m-ric Aymeric Roucher merve Merve Noyan albertvillanova Albert Villanova del Moral TL;DR Table of Contents Overview How we gave sight to smolagents How to create a Web browsing agent with vision Running the agent Next Steps You hypocrite, first take the log out of your own eye, and then you will see clearly to take the speck out of your brother's eye. Matthew 7, 3-5 TL;DR We have added vision support to smolagents, which unlocks the use of vision language models in agentic pipelines natively. Table of Contents Overview How we gave sight to smolagents How to create a Web browsing agent with vision Next Steps Overview In the agentic world, many capabilities are hidden behind a vision wall. A common example is web browsing: web pages feature rich visual content that you never fully recover by simply extracting their text, be it the relative position of objects, messages...", "url": "https://huggingface.co/blog/smolagents-can-see", "date_published": "2025-01-24T00:00:00"}, {"id": "https://huggingface.co/blog/smolervlm", "image": "https://huggingface.co/blog/assets/smolervlm/banner.png", "title": "SmolVLM Grows Smaller \u2013 Introducing the 250M & 500M Models!", "content_text": "Back to Articles SmolVLM Grows Smaller \u2013 Introducing the 250M & 500M Models! Published January 23, 2025 Update on GitHub Upvote 119 +113 andito Andres Marafioti mfarre Miquel Farr\u00e9 merve Merve Noyan TLDR Table of Contents Overview Why Go Smaller? Meet the 256M Parameter Giant A Step Up: 500M What Changed Since SmolVLM 2B? Smaller Multimodal Retrieval: ColSmolVLM 256M & 500M SmolDocling Using Smaller SmolVLM Next Steps TLDR We\u2019re excited to announce two new additions to the SmolVLM family: SmolVLM-256M and SmolVLM-500M. That\u2019s right\u2014256M parameters, making it the smallest Vision Language Model in the world! We built on everything we learned from SmolVLM 2B while focusing on efficiency, data mixtures, and new design trade-offs. We are excited to introduce a pair of models that preserve strong multimodal performance in a fraction of the footprint. This release comes with four checkpoints: two base models and two instruction fine-tuned models with sizes 256M and 500M parameters. These...", "url": "https://huggingface.co/blog/smolervlm", "date_published": "2025-01-23T00:00:00"}, {"id": "https://huggingface.co/blog/friendliai-partnership", "image": "https://huggingface.co/blog/assets/friendliai-partnership/thumbnail.png", "title": "Hugging Face and FriendliAI partner to supercharge model deployment on the Hub", "content_text": "Back to Articles Hugging Face and FriendliAI partner to supercharge model deployment on the Hub Published January 22, 2025 Update on GitHub Upvote 30 +24 ajshinfai Ahnjae Shin FriendliAI soominc Soomin Chun FriendliAI bgchun Byung-Gon Chun FriendliAI julien-c Julien Chaumond A Collaboration to Advance AI Innovation Simplifying Model Deployment Deploy models with NVIDIA H100 in Friendli Dedicated Endpoints Inference Open-Source Models with Friendli Serverless Endpoints What\u2019s Next FriendliAI\u2019s inference infrastructure is now integrated into the Hugging Face Hub as an option in the \u201cDeploy this model\u201d button, simplifying and accelerating generative AI model serving. A Collaboration to Advance AI Innovation Hugging Face empowers developers, researchers, and businesses to innovate in AI. Our common priority is building impactful partnerships that simplify workflows and provide cutting-edge tools for the AI community. Today, we are excited to announce a partnership between HF and...", "url": "https://huggingface.co/blog/friendliai-partnership", "date_published": "2025-01-22T00:00:00"}, {"id": "https://huggingface.co/blog/timm-transformers", "image": "https://huggingface.co/blog/assets/timm-transformers/thumbnail.png", "title": "Timm \u2764\ufe0f Transformers: Use any timm model with transformers", "content_text": "Back to Articles Timm \u2764\ufe0f Transformers: Use any timm model with transformers Published January 16, 2025 Update on GitHub Upvote 39 +33 ariG23498 Aritra Roy Gosthipaty rwightman Ross Wightman qubvel-hf Pavel Iakubovskii pcuenq Pedro Cuenca reach-vb Vaibhav Srivastav What is timm? Why Use the timm integration? Pipeline API: Using timm Models for Image Classification Gradio Integration: Building a Food Classifier Demo \ud83c\udf63 Auto Classes: Simplifying Model Loading Running quantized timm models Supervised Fine-Tuning of timm models Standard Fine-Tuning with the Trainer API LoRA Fine-Tuning for Efficient Training Inference with LoRA Fine-Tuned Model Round trip integration Torch Compile: Instant Speedup Wrapping Up Acknowledgments Get lightning-fast inference, quick quantization, torch.compile boosts, and effortless fine-tuning for any timm model\u2014all within the friendly \ud83e\udd17 transformers ecosystem. Enter TimmWrapper \u2014a simple, yet powerful tool that unlocks this potential. In this post, we\u2019ll...", "url": "https://huggingface.co/blog/timm-transformers", "date_published": "2025-01-16T00:00:00"}, {"id": "https://huggingface.co/blog/tgi-multi-backend", "image": "https://huggingface.co/blog/assets/tgi-multi-backend/thumbnail.png", "title": "Introducing multi-backends (TRT-LLM, vLLM) support for Text Generation Inference", "content_text": "Back to Articles Introducing multi-backends (TRT-LLM, vLLM) support for Text Generation Inference Published January 16, 2025 Update on GitHub Upvote 63 +57 mfuntowicz Morgan Funtowicz hlarcher Hugo Larcher Introduction TGI Backend: under the hood Looking forward: 2025 Introduction Since its initial release in 2022, Text-Generation-Inference (TGI) has provided Hugging Face and the AI Community with a performance-focused solution to easily deploy large-language models (LLMs). TGI initially offered an almost no-code solution to load models from the Hugging Face Hub and deploy them in production on NVIDIA GPUs. Over time, support expanded to include AMD Instinct GPUs, Intel GPUs, AWS Trainium/Inferentia, Google TPU, and Intel Gaudi. Over the years, multiple inferencing solutions have emerged, including vLLM, SGLang, llama.cpp, TensorRT-LLM, etc., splitting up the overall ecosystem. Different models, hardware, and use cases may require a specific backend to achieve optimal performance....", "url": "https://huggingface.co/blog/tgi-multi-backend", "date_published": "2025-01-16T00:00:00"}, {"id": "https://huggingface.co/blog/static-embeddings", "image": "https://huggingface.co/blog/assets/train-sentence-transformers/st-hf-thumbnail.png", "title": "Train 400x faster Static Embedding Models with Sentence Transformers", "content_text": "Back to Articles Train 400x faster Static Embedding Models with Sentence Transformers Published January 15, 2025 Update on GitHub Upvote 138 +132 tomaarsen Tom Aarsen TL;DR Table of Contents What are Embeddings? Modern Embeddings Static Embeddings Our Method Training Details Training Requirements Model Inspiration Training Dataset Selection Loss Function Selection Training Arguments Selection Evaluator Selection Hardware Details Overall Training Scripts Usage English Retrieval Multilingual Similarity Matryoshka Dimensionality Truncation Third Party libraries Performance English Retrieval Multilingual Similarity Conclusion Next Steps Acknowledgements TL;DR This blog post introduces a method to train static embedding models that run 100x to 400x faster on CPU than state-of-the-art embedding models, while retaining most of the quality. This unlocks a lot of exciting use cases, including on-device and in-browser execution, edge computing, low power and embedded applications. We apply...", "url": "https://huggingface.co/blog/static-embeddings", "date_published": "2025-01-15T00:00:00"}, {"id": "https://huggingface.co/blog/run-comfyui-workflows-on-spaces", "image": "https://huggingface.co/blog/assets/comfyui-to-gradio/cover.png", "title": "Run ComfyUI workflows for free on Spaces", "content_text": "Back to Articles Run ComfyUI workflows for free with Gradio on Hugging Face Spaces Published January 14, 2024 Update on GitHub Upvote 50 +44 multimodalart Apolin\u00e1rio from multimodal AI art cbensimon Charles Bensimon Intro Prerequisites 1. Exporting your ComfyUI workflow to run on pure Python 2. Create a Gradio app for the exported Python 3. Preparing it to run Hugging Face Spaces 4. Exporting to Spaces and running on ZeroGPU Fix requirements Move models outside the decorated function (ZeroGPU only) If you are not a PRO subscriber (skip this step if you are) The demo is running 5. Conclusion Index: Intro Prerequisites Exporting your ComfyUI workflow to run on pure Python Create a Gradio app for your ComfyUI app Prepare it to run on Hugging Face Spaces Exporting to Spaces and running on ZeroGPU Conclusion Intro In this tutorial I will present a step-by-step guide on how to convert a complex ComfyUI workflow to a simple Gradio application, and how to deploy this application on Hugging...", "url": "https://huggingface.co/blog/run-comfyui-workflows-on-spaces", "date_published": "2024-01-14T00:00:00"}]}