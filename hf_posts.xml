<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>The Hugging Face agents course is finally out!</title><link>https://huggingface.co/posts/burtenshaw/457613029588941</link><description>The Hugging Face agents course is finally out! 👉 https://huggingface.co/agents-course This first unit of the course sets you up with all the fundamentals to become a pro in agents. - What's an AI Agent? - What are LLMs? - Messages and Special Tokens - Understanding AI Agents through the Thought-Action-Observation Cycle - Thought, Internal Reasoning and the Re-Act Approach - Actions, Enabling the Agent to Engage with Its Environment - Observe, Integrating Feedback to Reflect and Adapt See translation</description><pubDate>Fri, 14 Feb 2025 13:26:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/burtenshaw/457613029588941</guid></item><item><title>"𝟮𝟬𝟮𝟱 𝘄𝗶𝗹𝗹 𝗯𝗲 𝘁𝗵𝗲 𝘆𝗲𝗮𝗿 𝗼𝗳 𝗔𝗜 𝗮𝗴𝗲𝗻𝘁𝘀": this statement has often been made, here are numbers to support it.</title><link>https://huggingface.co/posts/m-ric/116861695030454</link><description>"𝟮𝟬𝟮𝟱 𝘄𝗶𝗹𝗹 𝗯𝗲 𝘁𝗵𝗲 𝘆𝗲𝗮𝗿 𝗼𝗳 𝗔𝗜 𝗮𝗴𝗲𝗻𝘁𝘀": this statement has often been made, here are numbers to support it. I've plotted the progress of AI agents on GAIA test set, and it seems they're headed to catch up with the human baseline in early 2026. And that progress is still driven mostly by the improvement of base LLMs: progress would be even faster with fine-tuned agentic models. See translation</description><pubDate>Fri, 14 Feb 2025 13:26:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/m-ric/116861695030454</guid></item><item><title>Some things are simple</title><link>https://huggingface.co/posts/etemiz/440192103698875</link><description>Some things are simple See translation</description><pubDate>Fri, 14 Feb 2025 13:26:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/etemiz/440192103698875</guid></item><item><title>InspireMusic 🎵🔥 an open music generation framework by Alibaba FunAudio Lab</title><link>https://huggingface.co/posts/AdinaY/720327371561767</link><description>InspireMusic 🎵🔥 an open music generation framework by Alibaba FunAudio Lab Model: FunAudioLLM/InspireMusic-1.5B-Long Demo: FunAudioLLM/InspireMusic ✨ Music, songs, audio - ALL IN ONE ✨ High quality audio: 24kHz &amp; 48kHz sampling rates ✨ Long-Form Generation: enables extended audio creation ✨ Efficient Fine-Tuning: precision (BF16, FP16, FP32) with user-friendly scripts See translation</description><pubDate>Fri, 14 Feb 2025 13:26:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/720327371561767</guid></item><item><title>Toward the end of last year, the Xet team provided an inside look into the foundations of how we plan to enable rapid experimentation and iteration for the AI builders on the Hub:</title><link>https://huggingface.co/posts/jsulz/515376333515070</link><description>Toward the end of last year, the Xet team provided an inside look into the foundations of how we plan to enable rapid experimentation and iteration for the AI builders on the Hub: https://huggingface.co/blog/from-files-to-chunks But it turns out chunks aren't all you need! Our goal is to bring: 🚀 Faster uploads ⏬ Speedy downloads 💪 All without sacrificing your workflow To do that, we need the infrastructure and system and design to back it up. As we prepare to roll out the first Xet-backed repositories on the Hub, we wrote up a post explaining the nitty gritty details of the decisions that bring this to life https://huggingface.co/blog/from-chunks-to-blocks Complete with an interactive visualization that shows the power of deduplication in action - taking a 191GB repo to ~97GB and shaving a few hours off upload speeds. The darker each block in the heatmap, the more we dedupe, the less we have to transfer. Clicking on a file's blocks shows all other files that share blocks. Check it...</description><pubDate>Fri, 14 Feb 2025 13:26:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jsulz/515376333515070</guid></item><item><title>I've completed the first unit of the just-launched Hugging Face Agents Course. I would highly recommend it, even for experienced builders, because it is a great walkthrough of the smolagents library and toolkit.</title><link>https://huggingface.co/posts/ZennyKenny/584467772865203</link><description>I've completed the first unit of the just-launched Hugging Face Agents Course. I would highly recommend it, even for experienced builders, because it is a great walkthrough of the smolagents library and toolkit. See translation</description><pubDate>Fri, 14 Feb 2025 13:26:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ZennyKenny/584467772865203</guid></item><item><title>⭐️ The AI Energy Score project just launched - this is a game-changer for making informed decisions about AI deployment.</title><link>https://huggingface.co/posts/fdaudens/212771868233348</link><description>⭐️ The AI Energy Score project just launched - this is a game-changer for making informed decisions about AI deployment. You can now see exactly how much energy your chosen model will consume, with a simple 5-star rating system. Think appliance energy labels, but for AI. Looking at transcription models on the leaderboard is fascinating: choosing between whisper-tiny or whisper-large-v3 can make a 7x difference. Real-time data on these tradeoffs changes everything. 166 models already evaluated across 10 different tasks, from text generation to image classification. The whole thing is public and you can submit your own models to test. Why this matters: - Teams can pick efficient models that still get the job done - Developers can optimize for energy use from day one - Organizations can finally predict their AI environmental impact If you're building with AI at any scale, definitely worth checking out. 👉 leaderboard: https://lnkd.in/esrSxetj 👉 blog post: https://lnkd.in/eFJvzHi8 Huge...</description><pubDate>Fri, 14 Feb 2025 13:26:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/212771868233348</guid></item><item><title>I am excited to share that I’ve successfully completed Unit 1: Foundations of Agents in the Hugging Face Agents Course.</title><link>https://huggingface.co/posts/lukmanaj/991201381303884</link><description>I am excited to share that I’ve successfully completed Unit 1: Foundations of Agents in the Hugging Face Agents Course. Exploring the fundamentals of AI agents has been an insightful journey, and I’m looking forward to applying these concepts in real-world applications. Big thanks to the Hugging Face team for this amazing learning opportunity! 🤗 Check out the course here: https://huggingface.co/learn/agents-course/ See translation</description><pubDate>Fri, 14 Feb 2025 13:26:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/lukmanaj/991201381303884</guid></item><item><title>Runway Gen-3 Alpha: The Style and Coherence Champion</title><link>https://huggingface.co/posts/jasoncorkill/476446672223675</link><description>Runway Gen-3 Alpha: The Style and Coherence Champion Runway's latest video generation model, Gen-3 Alpha, is something special. It ranks #3 overall on our text-to-video human preference benchmark, but in terms of style and coherence, it outperforms even OpenAI Sora. However, it struggles with alignment, making it less predictable for controlled outputs. We've released a new dataset with human evaluations of Runway Gen-3 Alpha: Rapidata's text-2-video human preferences dataset. If you're working on video generation and want to see how your model compares to the biggest players, we can benchmark it for you. 🚀 DM us if you’re interested! Dataset: Rapidata/text-2-video-human-preferences-runway-alpha See translation</description><pubDate>Fri, 14 Feb 2025 13:26:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jasoncorkill/476446672223675</guid></item><item><title>Final upgrade to the Multi-Agent Task Completion Space:</title><link>https://huggingface.co/posts/CultriX/110138158069042</link><description>Final upgrade to the Multi-Agent Task Completion Space: CultriX/MultiAgent-CodeTask . It now includes : - a live stream of the progress being made on the task (see included video), - The following components: 1. Automatic prompt optimization 2. An orchestrator deciding which agent to call dynamically including feedback from a human (human-in-the-loop) 3. A coding agent to complete the task 4. A code reviewing agent to iteratively provide feedback to improve the code generated by the coding agent until the code meets the required criteria after which it is approved. 5. A testing agent that tests the approved code or provides information on how to test it. 6. A documentation agent that provides documentation and a help message for the approved and tested code. See translation</description><pubDate>Fri, 14 Feb 2025 13:26:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/CultriX/110138158069042</guid></item></channel></rss>