<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>I wrote an article about G2P:</title><link>https://huggingface.co/posts/hexgrad/562263062112849</link><description>I wrote an article about G2P: https://hf.co/blog/hexgrad/g2p G2P is an underrated piece of small TTS models, like offensive linemen who do a bunch of work and get no credit. Instead of relying on explicit G2P, larger speech models implicitly learn this task by eating many thousands of hours of audio data. They often use a 500M+ parameter LLM at the front to predict latent audio tokens over a learned codebook, then decode these tokens into audio. Kokoro instead relies on G2P preprocessing, is 82M parameters, and thus needs less audio to learn. Because of this, we can cherrypick high fidelity audio for training data, and deliver solid speech for those voices. In turn, this excellent audio quality &amp; lack of background noise helps explain why Kokoro is very competitive in single-voice TTS Arenas. See translation</description><pubDate>Fri, 07 Feb 2025 12:33:25 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hexgrad/562263062112849</guid></item><item><title>Introducing 𝗼𝗽𝗲𝗻 𝗗𝗲𝗲𝗽-𝗥𝗲𝘀𝗲𝗮𝗿𝗰𝗵 by Hugging Face! 💥</title><link>https://huggingface.co/posts/m-ric/410805194640777</link><description>Introducing 𝗼𝗽𝗲𝗻 𝗗𝗲𝗲𝗽-𝗥𝗲𝘀𝗲𝗮𝗿𝗰𝗵 by Hugging Face! 💥 OpenAI's latest agentic app Deep Research seems really good... But it's closed, as usual. ⏱️ So with a team of cracked colleagues, we set ourselves a 24hours deadline to replicate and open-source Deep Research! ⏱️ ➡️ We built open-Deep-Research, an entirely open agent that can: navigate the web autonomously, scroll and search through pages, download and manipulate files, run calculation on data... We aimed for the best performance: are the agent's answers really rigorous? On GAIA benchmark, Deep Research had 67% accuracy on the validation set. ➡️ open Deep Research is at 55% (powered by o1), it is: - the best pass@1 solution submitted - the best open solution 💪💪 And it's only getting started ! Please jump in, drop PRs, and let's bring it to the top ! Read the blog post 👉 https://huggingface.co/blog/open-deep-research See translation</description><pubDate>Fri, 07 Feb 2025 12:33:25 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/m-ric/410805194640777</guid></item><item><title>🚀 Excited to share that our paper, "SELP: Generating Safe and Efficient Task Plans for Robot Agents with Large Language Models", has been accepted to #ICRA2025! 🔗 Preprint:</title><link>https://huggingface.co/posts/lin-tan/763019179488759</link><description>🚀 Excited to share that our paper, "SELP: Generating Safe and Efficient Task Plans for Robot Agents with Large Language Models", has been accepted to #ICRA2025! 🔗 Preprint: https://arxiv.org/pdf/2409.19471 We introduce SELP (Safe Efficient LLM Planner), a novel approach for generating plans that adhere to user-specified constraints while optimizing for time-efficient execution. By leveraging linear temporal logic (LTL) to interpret natural language commands, SELP effectively handles complex commands and long-horizon tasks. 🤖 💡SELP presents three key insights: 1️⃣ Equivalence Voting: Ensures robust translations from natural language instructions into LTL specifications. 2️⃣ Constrained Decoding: Uses the generated LTL formula to guide the autoregressive inference of plans, ensuring the generated plans conform to the LTL. 3️⃣ Domain-Specific Fine-Tuning: Customizes LLMs for specific robotic tasks, boosting both safety and efficiency. 📊 Experiment: Our experiments demonstrate SELP’s...</description><pubDate>Fri, 07 Feb 2025 12:33:25 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/lin-tan/763019179488759</guid></item><item><title>Colox, a reasoning AI model. I am currently working on a model smarter than GPT o1 that thinks before it speaks. It is coming tomorrow in the afternoon.</title><link>https://huggingface.co/posts/retronic/114797173531800</link><description>Colox, a reasoning AI model. I am currently working on a model smarter than GPT o1 that thinks before it speaks. It is coming tomorrow in the afternoon. See translation</description><pubDate>Fri, 07 Feb 2025 12:33:25 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/retronic/114797173531800</guid></item><item><title>Xwen 🔥 a series of open models based on Qwen2.5 models, developed by a brilliant research team of PhD students from the Chinese community.</title><link>https://huggingface.co/posts/AdinaY/234944926747608</link><description>Xwen 🔥 a series of open models based on Qwen2.5 models, developed by a brilliant research team of PhD students from the Chinese community. shenzhi-wang/xwen-chat-679e30ab1f4b90cfa7dbc49e ✨ 7B/72B ✨ Apache 2.0 ✨ Xwen-72B-Chat outperformed DeepSeek V3 on Arena Hard Auto See translation</description><pubDate>Fri, 07 Feb 2025 12:33:25 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/234944926747608</guid></item><item><title>🚀 Introducing</title><link>https://huggingface.co/posts/albertvillanova/425238785566061</link><description>🚀 Introducing @ huggingface Open Deep-Research💥 In just 24 hours, we built an open-source agent that: ✅ Autonomously browse the web ✅ Search, scroll &amp; extract info ✅ Download &amp; manipulate files ✅ Run calculations on data 55% on GAIA validation set! Help us improve it!💡 https://huggingface.co/blog/open-deep-research See translation</description><pubDate>Fri, 07 Feb 2025 12:33:25 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/albertvillanova/425238785566061</guid></item><item><title>📱 UI Navigation Corpus -</title><link>https://huggingface.co/posts/nyuuzyou/794560065961999</link><description>📱 UI Navigation Corpus - teleren/ui-navigation-corpus A comprehensive collection of mobile and web UI elements created by a new member of the Hugging Face community @ teleren . I'm glad that I was able to provide a little help together with @ its5Q to get this dataset published. This dataset contains: - Screenshots and recordings of mobile (iOS/Android) and web interfaces - UI navigation annotations and metadata - Screen categorization tags and text extractions - Navigation paths and screen relationships - Version control for UI imagery Perfect for training UI navigation agents and understanding interface patterns. The dataset provides detailed annotations linking screens, sections, and navigation flows together. See translation</description><pubDate>Fri, 07 Feb 2025 12:33:25 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nyuuzyou/794560065961999</guid></item><item><title>Happy to announce AlignVLM 📏 – a novel approach to bridging vision and language latent spaces for multimodal understanding in Vision-Language Models (VLMs) 🌍📄🖼</title><link>https://huggingface.co/posts/ahmed-masry/423590538819866</link><description>Happy to announce AlignVLM 📏 – a novel approach to bridging vision and language latent spaces for multimodal understanding in Vision-Language Models (VLMs) 🌍📄🖼 🔗 Read the paper: AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding (2502.01341) 🧐 What’s the challenge? Aligning visual features with language embeddings remains a major bottleneck in VLMs. Existing connectors such as Multi-layer perceptron (MLPs) often introduce noise that degrades performance. ❌ 🎯 Our Solution: ALIGN Connector We propose AlignVLM, a method that maps vision features into a weighted average of LLM text embeddings, ensuring they remain in a space that the LLM can effectively interpret. ✅ 🔬 How does it perform? We compared ALIGN against common connectors like MLPs, Perceiver Resampler, and Ovis trained under similar configurations. The results? ALIGN outperforms them all 🏆 on diverse document understanding tasks 📄. 📊 Meet the AlignVLM Model Family! We trained Llama 3.1 (1B, 3B,...</description><pubDate>Fri, 07 Feb 2025 12:33:25 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ahmed-masry/423590538819866</guid></item><item><title>Hey everyone, we've given</title><link>https://huggingface.co/posts/victor/435864388294574</link><description>Hey everyone, we've given https://hf.co/spaces page a fresh update! Smart Search: Now just type what you want to do—like "make a viral meme" or "generate music"—and our search gets it. New Categories: Check out the cool new filter bar with icons to help you pick a category fast. Redesigned Space Cards: Reworked a bit to really show off the app descriptions, so you know what each Space does at a glance. Random Prompt: Need ideas? Hit the dice button for a burst of inspiration. We’d love to hear what you think—drop us some feedback plz! See translation</description><pubDate>Fri, 07 Feb 2025 12:33:25 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/victor/435864388294574</guid></item><item><title>We’re launching a FREE and CERTIFIED course on Agents!</title><link>https://huggingface.co/posts/burtenshaw/334573649974058</link><description>We’re launching a FREE and CERTIFIED course on Agents! We're thrilled to announce the launch of the Hugging Face Agents course on Learn! This interactive, certified course will guide you through building and deploying your own AI agents. Here's what you'll learn: - Understanding Agents: We'll break down the fundamentals of AI agents, showing you how they use LLMs to perceive their environment (observations), reason about it (thoughts), and take actions. Think of a smart assistant that can book appointments, answer emails, or even write code based on your instructions. - Building with Frameworks: You'll dive into popular agent frameworks like LangChain, LlamaIndex and smolagents. These tools provide the building blocks for creating complex agent behaviors. - Real-World Applications: See how agents are used in practice, from automating SQL queries to generating code and summarizing complex documents. - Certification: Earn a certification by completing the course modules, implementing...</description><pubDate>Fri, 07 Feb 2025 12:33:25 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/burtenshaw/334573649974058</guid></item></channel></rss>